{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1fe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                 | 1/100 [00:00<00:48,  2.03it/s][18:33:44] SMILES Parse Error: ring closure 2 duplicates bond between atom 4 and atom 5 for input: '*c1c[nH]n2c2c(F)cccc12'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:29<00:00,  3.43it/s]\n",
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.8484848484848485 Quality: 0.41 SA: 4.082602312947524 QED: 0.7548358677335902 diversity: 0.5818713929911064 distance: 0.8770998645193947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.63 Quality: 0.32 SA: 3.5770179738832515 QED: 0.5876661552588345 diversity: 0.4314305540518022 distance: 0.8798861931212343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9489795918367347 Quality: 0.22 SA: 4.073123692669797 QED: 0.48954089654286553 diversity: 0.7215945300918065 distance: 0.8996018386891813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:46<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.95 uniqueness: 0.9789473684210527 Quality: 0.29 SA: 3.824050751402876 QED: 0.5663470609936114 diversity: 0.7254525779134393 distance: 0.8864769578345574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.28 Quality: 0.21 SA: 3.426947587413814 QED: 0.6441973739660686 diversity: 0.266586589768544 distance: 0.9038154540013067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.97 uniqueness: 1.0 Quality: 0.84 SA: 2.911012857556073 QED: 0.7610462211755915 diversity: 0.7199658560547575 distance: 0.9044153334228633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.98989898989899 Quality: 0.59 SA: 3.2409333606627007 QED: 0.6192221836286511 diversity: 0.741413627654963 distance: 0.8830181443746858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:34<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9387755102040817 Quality: 0.0 SA: 4.881448244351062 QED: 0.40276351752939127 diversity: 0.47239707635483164 distance: 0.8481383638570643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:18<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.5 Quality: 0.3 SA: 2.9627928098030543 QED: 0.6241576441950124 diversity: 0.48271744221120644 distance: 0.8685142773081145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:53<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.91 uniqueness: 1.0 Quality: 0.16 SA: 3.6138539754333867 QED: 0.4124903346694743 diversity: 0.8308326803379792 distance: 0.9000781222485721\n",
      "运行时间: 303.6905 秒\n",
      "valid_ratio_avg: 0.977, uniqueness_avg: 0.8115086308845708, quality_avg: 0.4538263216399945, sa_avg: 3.659378356612354, qed_avg: 0.586226725569309, div_avg: 0.5974262327430436, dist_avg: 0.8851044549376974\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from dataset import SmileDataset, SmileCollator\n",
    "from tokenizer import SmilesTokenizer\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "# from test_connet import reconstruct\n",
    "from fragment_utils import reconstruct\n",
    "from tqdm import tqdm\n",
    "from utils.train_utils import seed_all\n",
    "from tdc import Oracle\n",
    "\n",
    "\n",
    "def calculate_tanimoto_distance(fingerprint1, fingerprint2):\n",
    "    \"\"\"\n",
    "    计算两个指纹之间的 Tanimoto 距离。\n",
    "    \"\"\"\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fingerprint1, fingerprint2)\n",
    "\n",
    "def calculate_morgan_fingerprint(mol, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算分子的 Morgan 指纹。\n",
    "    Args:\n",
    "        mol: RDKit 分子对象。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        分子指纹，或者如果分子无效则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_diversity(molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子的多样性（平均成对 Tanimoto 距离）。\n",
    "    Args:\n",
    "        molecules: RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        多样性值。\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_molecules = []\n",
    "    for mol in molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            fingerprints.append(fp)\n",
    "            valid_molecules.append(mol)\n",
    "    if not fingerprints:\n",
    "        return 0.0  # 如果没有有效分子，返回 0.0\n",
    "    n = len(fingerprints)\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = calculate_tanimoto_distance(fingerprints[i], fingerprints[j])\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def calculate_distance(generated_molecules, original_molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子与原始分子之间的平均 Tanimoto 距离。\n",
    "    Args:\n",
    "        generated_molecules: 生成的 RDKit 分子对象的列表。\n",
    "        original_molecules: 原始 RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        平均距离值。\n",
    "    \"\"\"\n",
    "    generated_fingerprints = []\n",
    "    original_fingerprints = []\n",
    "    # 计算生成分子的指纹\n",
    "    for mol in generated_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            generated_fingerprints.append(fp)\n",
    "    # 计算原始分子的指纹\n",
    "    for mol in original_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            original_fingerprints.append(fp)\n",
    "    if not generated_fingerprints or not original_fingerprints:\n",
    "        return 0.0\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for gen_fp in generated_fingerprints:\n",
    "        for orig_fp in original_fingerprints:\n",
    "            distance = calculate_tanimoto_distance(gen_fp, orig_fp)\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "\n",
    "def cal_QED(smiles):\n",
    "    oracle = Oracle(name = 'QED')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_SA(smiles):\n",
    "    oracle = Oracle(name = 'SA')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_all(smiles):\n",
    "    results = {}\n",
    "    results['QED'] = cal_QED(smiles)\n",
    "    results['SA'] = cal_SA(smiles)\n",
    "    return results\n",
    "\n",
    "\n",
    "def Test(model, smiles, tokenizer, max_seq_len, temperature, top_k, stream, rp, num_samples, kv_cache, is_simulation,\n",
    "         device, scaffold=False, linker=False):\n",
    "    complete_answer_list = []\n",
    "    valid_answer_list = []\n",
    "    model.eval()\n",
    "    # place data on the correct device\n",
    "    src_smiles = tokenizer.bos_token + smiles\n",
    "    x = torch.tensor(tokenizer.encode(src_smiles, add_special_tokens=False), dtype=torch.long).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        res_y = model.generate(x, tokenizer, max_new_tokens=max_seq_len,\n",
    "                               temperature=temperature, top_k=top_k, stream=stream, rp=rp, kv_cache=kv_cache,\n",
    "                               is_simulation=is_simulation)\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "\n",
    "        history_idx = 0\n",
    "        complete_answer = f\"{tokenizer.decode(x[0])}\"  # 用于保存整个生成的句子\n",
    "\n",
    "        while y != None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "            if not len(answer):\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 保存生成的片段到完整回答中\n",
    "            complete_answer += answer[history_idx:]\n",
    "\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "            if not stream:\n",
    "                break\n",
    "\n",
    "        complete_answer = complete_answer.replace(\" \", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "        frag_list = complete_answer.replace(\" \", \"\").split('[SEP]')\n",
    "        try:\n",
    "            if linker:\n",
    "                last_frag = frag_list[0].split('.')[1]\n",
    "                first_frag = frag_list[0].split('.')[0]\n",
    "                frag_list[0] = first_frag\n",
    "                frag_list[len(frag_list) - 1] = last_frag\n",
    "            frag_mol = [Chem.MolFromSmiles(s) for s in frag_list]\n",
    "            mol = reconstruct(frag_mol)[0]\n",
    "            if type(mol) == list:\n",
    "                mol = mol[0]\n",
    "            if mol:\n",
    "                generate_smiles = Chem.MolToSmiles(mol)\n",
    "                valid_answer_list.append(generate_smiles)\n",
    "                answer = frag_list\n",
    "            else:\n",
    "                answer = frag_list\n",
    "        except:\n",
    "            answer = frag_list\n",
    "        complete_answer_list.append(answer)\n",
    "\n",
    "    return complete_answer_list, valid_answer_list\n",
    "\n",
    "def main_motif():\n",
    "    motif_lst = ['*N1CC2(C[C@H]1C(=O)O)SCCS2[SEP]', '*C1Nc2cc(Cl)c(S(N)(=O)=O)cc2S(=O)(=O)N1[SEP]',\n",
    "     '*[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O[SEP]', '*[C@H]1CCN(C(=O)C=C)C1[SEP]', '*C1(CC#N)CN(S(=O)(=O)CC)C1[SEP]',\n",
    "     '*c1ccc2c(c1)OCCO2[SEP]', '*C[C@H](N)C(=O)O[SEP]',\n",
    "     '*[C@@H]1[C@@H]2C(=C[C@H](C)C[C@@H]2OC(=O)[C@@H](C)CC)C=C[C@@H]1C[SEP]', '*n1c(Br)nnc1SCC(=O)O[SEP]', '*OCCOC[SEP]']\n",
    "    original_smiles = ['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CC2(C[C@H]1C(=O)O)SCCS2',\n",
    "     'NS(=O)(=O)c1cc2c(cc1Cl)NC(C1CC3C=CC1C3)NS2(=O)=O', 'CC(C)Nc1nc2cc(Cl)c(Cl)cc2n1[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O',\n",
    "     'C=CC(=O)N1CC[C@H](n2nc(C#Cc3cc(OC)cc(OC)c3)c3c(N)ncnc32)C1',\n",
    "     'CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1', 'CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1)OCCO2',\n",
    "     'N[C@@H](Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1)C(=O)O',\n",
    "     'CC[C@H](C)C(=O)O[C@H]1C[C@@H](C)C=C2C=C[C@H](C)[C@H](CC[C@@H]3C[C@@H](O)CC(=O)O3)[C@H]21',\n",
    "     'O=C(O)CSc1nnc(Br)n1-c1ccc(C2CC2)c2ccccc12', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1']\n",
    "\n",
    "    # 设置随机种子的值\n",
    "    seed_value = 42\n",
    "    seed_all(seed_value)\n",
    "    # device = torch.device(f'cuda:{0}')  # 逻辑编号 cuda:0 对应 os.environ[\"CUDA_VISIBLE_DEVICES\"]中的第一个gpu\n",
    "    device = 'cuda:8'\n",
    "    batch_size = 1\n",
    "\n",
    "    test_names = \"test\"\n",
    "\n",
    "    tokenizer = SmilesTokenizer('./vocabs/vocab.txt')\n",
    "    tokenizer.bos_token = \"[BOS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"[BOS]\")\n",
    "    tokenizer.eos_token = \"[EOS]\"\n",
    "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
    "\n",
    "    mconf = GPTConfig(vocab_size=tokenizer.vocab_size, n_layer=12, n_head=12, n_embd=768)\n",
    "    model = GPT(mconf).to(device)\n",
    "    checkpoint = torch.load(f'./weights/fragpt.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    start_time = time.time()\n",
    "    valid_ratio_sum = 0\n",
    "    uniqueness_sum = 0\n",
    "    quality_sum = 0\n",
    "    sa_sum = 0\n",
    "    qed_sum = 0\n",
    "    div_sum = 0\n",
    "    dist_sum = 0\n",
    "    for i in motif_lst:\n",
    "        complete_answer_list, valid_answer_list = [], []\n",
    "        for j in tqdm(range(100)):\n",
    "            l1, l2 = Test(model, i, tokenizer, max_seq_len=512, temperature=1.2, top_k=8, stream=False, rp=1., num_samples=1,\n",
    "                 kv_cache=True, is_simulation=True, device=device)\n",
    "            if (len(l2) != 0):\n",
    "                valid_answer_list.append(l2[0])\n",
    "            if (len(l1) != 0):\n",
    "                complete_answer_list.append(l1[0])\n",
    "        unique_smiles = set(smile for smile in valid_answer_list if smile is not None)\n",
    "        unique_smiles_lst = list(unique_smiles)\n",
    "        num_unique_molecules = len(unique_smiles)\n",
    "        uniqueness = num_unique_molecules / len(valid_answer_list)\n",
    "        valid_ratio = len(valid_answer_list) / 100\n",
    "        results = cal_all(unique_smiles_lst)\n",
    "        SA_score = 0\n",
    "        QED_score = 0\n",
    "        sum = 0\n",
    "        for k in range(len(unique_smiles_lst)):\n",
    "            SA_score += results['SA'][k]\n",
    "            QED_score += results['QED'][k]\n",
    "            if (results['QED'][k] >= 0.6 and results['SA'][k] <= 4):\n",
    "                sum += 1\n",
    "\n",
    "        generated_molecules = [Chem.MolFromSmiles(s) for s in valid_answer_list]\n",
    "        original_molecules = [Chem.MolFromSmiles(s) for s in original_smiles]\n",
    "        # 计算多样性\n",
    "        diversity = calculate_diversity(generated_molecules)\n",
    "        # 计算距离\n",
    "        distance = calculate_distance(generated_molecules, original_molecules)\n",
    "\n",
    "        print('valid_ratio:', valid_ratio, 'uniqueness:', uniqueness, 'Quality:', sum / 100, 'SA:',\n",
    "              SA_score / len(unique_smiles_lst), 'QED:', QED_score / len(unique_smiles_lst), 'diversity:', diversity,\n",
    "              'distance:', distance)\n",
    "        valid_ratio_sum += valid_ratio\n",
    "        uniqueness_sum += uniqueness\n",
    "        quality_sum += sum / len(unique_smiles_lst)\n",
    "        sa_sum += SA_score / len(unique_smiles_lst)\n",
    "        qed_sum += QED_score / len(unique_smiles_lst)\n",
    "        div_sum += diversity\n",
    "        dist_sum += distance\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"运行时间: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"valid_ratio_avg: {valid_ratio_sum / len(motif_lst)}, uniqueness_avg: {uniqueness_sum / len(motif_lst)}, \"\n",
    "          f\"quality_avg: {quality_sum / len(motif_lst)}, sa_avg: {sa_sum / len(motif_lst)}, \"\n",
    "          f\"qed_avg: {qed_sum / len(motif_lst)}, div_avg: {div_sum / len(motif_lst)}, dist_avg: {dist_sum / len(motif_lst)}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main_motif()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e75c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.7653061224489796 Quality: 0.37 SA: 4.0471873052531775 QED: 0.7545465261416803 diversity: 0.5633223096235295 distance: 0.8755049484706365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.75 Quality: 0.33 SA: 3.6392827088843402 QED: 0.5561704435118435 diversity: 0.4691635497757367 distance: 0.8790862775417675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.95 uniqueness: 0.9368421052631579 Quality: 0.16 SA: 4.167056595675648 QED: 0.43220426643167703 diversity: 0.7238316485244194 distance: 0.9004734953375509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:44<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.97 uniqueness: 0.9896907216494846 Quality: 0.33 SA: 3.790744183686355 QED: 0.5666686542736127 diversity: 0.7204393772245676 distance: 0.8851340754706231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.29 Quality: 0.17 SA: 3.5619251530493647 QED: 0.5910098977371305 diversity: 0.32714394711850625 distance: 0.9019588589778192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:30<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.96 uniqueness: 1.0 Quality: 0.75 SA: 2.937193663879391 QED: 0.7333701140492966 diversity: 0.7308327962811164 distance: 0.9061918642383304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:29<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 1.0 Quality: 0.6 SA: 3.2830916427822614 QED: 0.6376612707650426 diversity: 0.748315603562503 distance: 0.8809779597420602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.9393939393939394 Quality: 0.0 SA: 4.779432262350212 QED: 0.41672534977475667 diversity: 0.4424315743066368 distance: 0.8460635319532098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.53 Quality: 0.39 SA: 2.76748766119774 QED: 0.7265237963365047 diversity: 0.474277436289222 distance: 0.8669336765950608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.88 uniqueness: 1.0 Quality: 0.3 SA: 3.5823658020208713 QED: 0.46680602974331475 diversity: 0.825303516549678 distance: 0.89642818776501\n",
      "运行时间: 304.0237 秒\n",
      "valid_ratio_avg: 0.9710000000000001, uniqueness_avg: 0.8201232888755563, quality_avg: 0.4513318556255982, sa_avg: 3.655576697877936, qed_avg: 0.5881686348764859, div_avg: 0.6025061759255916, dist_avg: 0.8838752876092066\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from dataset import SmileDataset, SmileCollator\n",
    "from tokenizer import SmilesTokenizer\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "# from test_connet import reconstruct\n",
    "from fragment_utils import reconstruct\n",
    "from tqdm import tqdm\n",
    "from utils.train_utils import seed_all\n",
    "from tdc import Oracle\n",
    "\n",
    "\n",
    "def calculate_tanimoto_distance(fingerprint1, fingerprint2):\n",
    "    \"\"\"\n",
    "    计算两个指纹之间的 Tanimoto 距离。\n",
    "    \"\"\"\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fingerprint1, fingerprint2)\n",
    "\n",
    "def calculate_morgan_fingerprint(mol, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算分子的 Morgan 指纹。\n",
    "    Args:\n",
    "        mol: RDKit 分子对象。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        分子指纹，或者如果分子无效则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_diversity(molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子的多样性（平均成对 Tanimoto 距离）。\n",
    "    Args:\n",
    "        molecules: RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        多样性值。\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_molecules = []\n",
    "    for mol in molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            fingerprints.append(fp)\n",
    "            valid_molecules.append(mol)\n",
    "    if not fingerprints:\n",
    "        return 0.0  # 如果没有有效分子，返回 0.0\n",
    "    n = len(fingerprints)\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = calculate_tanimoto_distance(fingerprints[i], fingerprints[j])\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def calculate_distance(generated_molecules, original_molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子与原始分子之间的平均 Tanimoto 距离。\n",
    "    Args:\n",
    "        generated_molecules: 生成的 RDKit 分子对象的列表。\n",
    "        original_molecules: 原始 RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        平均距离值。\n",
    "    \"\"\"\n",
    "    generated_fingerprints = []\n",
    "    original_fingerprints = []\n",
    "    # 计算生成分子的指纹\n",
    "    for mol in generated_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            generated_fingerprints.append(fp)\n",
    "    # 计算原始分子的指纹\n",
    "    for mol in original_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            original_fingerprints.append(fp)\n",
    "    if not generated_fingerprints or not original_fingerprints:\n",
    "        return 0.0\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for gen_fp in generated_fingerprints:\n",
    "        for orig_fp in original_fingerprints:\n",
    "            distance = calculate_tanimoto_distance(gen_fp, orig_fp)\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "\n",
    "def cal_QED(smiles):\n",
    "    oracle = Oracle(name = 'QED')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_SA(smiles):\n",
    "    oracle = Oracle(name = 'SA')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_all(smiles):\n",
    "    results = {}\n",
    "    results['QED'] = cal_QED(smiles)\n",
    "    results['SA'] = cal_SA(smiles)\n",
    "    return results\n",
    "\n",
    "\n",
    "def Test(model, smiles, tokenizer, max_seq_len, temperature, top_k, stream, rp, num_samples, kv_cache, is_simulation,\n",
    "         device, scaffold=False, linker=False):\n",
    "    complete_answer_list = []\n",
    "    valid_answer_list = []\n",
    "    model.eval()\n",
    "    # place data on the correct device\n",
    "    src_smiles = tokenizer.bos_token + smiles\n",
    "    x = torch.tensor(tokenizer.encode(src_smiles, add_special_tokens=False), dtype=torch.long).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        res_y = model.generate(x, tokenizer, max_new_tokens=max_seq_len,\n",
    "                               temperature=temperature, top_k=top_k, stream=stream, rp=rp, kv_cache=kv_cache,\n",
    "                               is_simulation=is_simulation)\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "\n",
    "        history_idx = 0\n",
    "        complete_answer = f\"{tokenizer.decode(x[0])}\"  # 用于保存整个生成的句子\n",
    "\n",
    "        while y != None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "            if not len(answer):\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 保存生成的片段到完整回答中\n",
    "            complete_answer += answer[history_idx:]\n",
    "\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "            if not stream:\n",
    "                break\n",
    "\n",
    "        complete_answer = complete_answer.replace(\" \", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "        frag_list = complete_answer.replace(\" \", \"\").split('[SEP]')\n",
    "        try:\n",
    "            if linker:\n",
    "                last_frag = frag_list[0].split('.')[1]\n",
    "                first_frag = frag_list[0].split('.')[0]\n",
    "                frag_list[0] = first_frag\n",
    "                frag_list[len(frag_list) - 1] = last_frag\n",
    "            frag_mol = [Chem.MolFromSmiles(s) for s in frag_list]\n",
    "            mol = reconstruct(frag_mol)[0]\n",
    "            if type(mol) == list:\n",
    "                mol = mol[0]\n",
    "            if mol:\n",
    "                generate_smiles = Chem.MolToSmiles(mol)\n",
    "                valid_answer_list.append(generate_smiles)\n",
    "                answer = frag_list\n",
    "            else:\n",
    "                answer = frag_list\n",
    "        except:\n",
    "            answer = frag_list\n",
    "        complete_answer_list.append(answer)\n",
    "\n",
    "    return complete_answer_list, valid_answer_list\n",
    "\n",
    "def main_motif():\n",
    "    motif_lst = ['*N1CC2(C[C@H]1C(=O)O)SCCS2[SEP]', '*C1Nc2cc(Cl)c(S(N)(=O)=O)cc2S(=O)(=O)N1[SEP]',\n",
    "     '*[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O[SEP]', '*[C@H]1CCN(C(=O)C=C)C1[SEP]', '*C1(CC#N)CN(S(=O)(=O)CC)C1[SEP]',\n",
    "     '*c1ccc2c(c1)OCCO2[SEP]', '*C[C@H](N)C(=O)O[SEP]',\n",
    "     '*[C@@H]1[C@@H]2C(=C[C@H](C)C[C@@H]2OC(=O)[C@@H](C)CC)C=C[C@@H]1C[SEP]', '*n1c(Br)nnc1SCC(=O)O[SEP]', '*OCCOC[SEP]']\n",
    "    original_smiles = ['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CC2(C[C@H]1C(=O)O)SCCS2',\n",
    "     'NS(=O)(=O)c1cc2c(cc1Cl)NC(C1CC3C=CC1C3)NS2(=O)=O', 'CC(C)Nc1nc2cc(Cl)c(Cl)cc2n1[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O',\n",
    "     'C=CC(=O)N1CC[C@H](n2nc(C#Cc3cc(OC)cc(OC)c3)c3c(N)ncnc32)C1',\n",
    "     'CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1', 'CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1)OCCO2',\n",
    "     'N[C@@H](Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1)C(=O)O',\n",
    "     'CC[C@H](C)C(=O)O[C@H]1C[C@@H](C)C=C2C=C[C@H](C)[C@H](CC[C@@H]3C[C@@H](O)CC(=O)O3)[C@H]21',\n",
    "     'O=C(O)CSc1nnc(Br)n1-c1ccc(C2CC2)c2ccccc12', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1']\n",
    "\n",
    "    # 设置随机种子的值\n",
    "    seed_value = 43\n",
    "    seed_all(seed_value)\n",
    "    # device = torch.device(f'cuda:{0}')  # 逻辑编号 cuda:0 对应 os.environ[\"CUDA_VISIBLE_DEVICES\"]中的第一个gpu\n",
    "    device = 'cuda:8'\n",
    "    batch_size = 1\n",
    "\n",
    "    test_names = \"test\"\n",
    "\n",
    "    tokenizer = SmilesTokenizer('./vocabs/vocab.txt')\n",
    "    tokenizer.bos_token = \"[BOS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"[BOS]\")\n",
    "    tokenizer.eos_token = \"[EOS]\"\n",
    "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
    "\n",
    "    mconf = GPTConfig(vocab_size=tokenizer.vocab_size, n_layer=12, n_head=12, n_embd=768)\n",
    "    model = GPT(mconf).to(device)\n",
    "    checkpoint = torch.load(f'./weights/fragpt.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    start_time = time.time()\n",
    "    valid_ratio_sum = 0\n",
    "    uniqueness_sum = 0\n",
    "    quality_sum = 0\n",
    "    sa_sum = 0\n",
    "    qed_sum = 0\n",
    "    div_sum = 0\n",
    "    dist_sum = 0\n",
    "    for i in motif_lst:\n",
    "        complete_answer_list, valid_answer_list = [], []\n",
    "        for j in tqdm(range(100)):\n",
    "            l1, l2 = Test(model, i, tokenizer, max_seq_len=512, temperature=1.2, top_k=8, stream=False, rp=1., num_samples=1,\n",
    "                 kv_cache=True, is_simulation=True, device=device)\n",
    "            if (len(l2) != 0):\n",
    "                valid_answer_list.append(l2[0])\n",
    "            if (len(l1) != 0):\n",
    "                complete_answer_list.append(l1[0])\n",
    "        unique_smiles = set(smile for smile in valid_answer_list if smile is not None)\n",
    "        unique_smiles_lst = list(unique_smiles)\n",
    "        num_unique_molecules = len(unique_smiles)\n",
    "        uniqueness = num_unique_molecules / len(valid_answer_list)\n",
    "        valid_ratio = len(valid_answer_list) / 100\n",
    "        results = cal_all(unique_smiles_lst)\n",
    "        SA_score = 0\n",
    "        QED_score = 0\n",
    "        sum = 0\n",
    "        for k in range(len(unique_smiles_lst)):\n",
    "            SA_score += results['SA'][k]\n",
    "            QED_score += results['QED'][k]\n",
    "            if (results['QED'][k] >= 0.6 and results['SA'][k] <= 4):\n",
    "                sum += 1\n",
    "\n",
    "        generated_molecules = [Chem.MolFromSmiles(s) for s in valid_answer_list]\n",
    "        original_molecules = [Chem.MolFromSmiles(s) for s in original_smiles]\n",
    "        # 计算多样性\n",
    "        diversity = calculate_diversity(generated_molecules)\n",
    "        # 计算距离\n",
    "        distance = calculate_distance(generated_molecules, original_molecules)\n",
    "\n",
    "        print('valid_ratio:', valid_ratio, 'uniqueness:', uniqueness, 'Quality:', sum / 100, 'SA:',\n",
    "              SA_score / len(unique_smiles_lst), 'QED:', QED_score / len(unique_smiles_lst), 'diversity:', diversity,\n",
    "              'distance:', distance)\n",
    "        valid_ratio_sum += valid_ratio\n",
    "        uniqueness_sum += uniqueness\n",
    "        quality_sum += sum / len(unique_smiles_lst)\n",
    "        sa_sum += SA_score / len(unique_smiles_lst)\n",
    "        qed_sum += QED_score / len(unique_smiles_lst)\n",
    "        div_sum += diversity\n",
    "        dist_sum += distance\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"运行时间: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"valid_ratio_avg: {valid_ratio_sum / len(motif_lst)}, uniqueness_avg: {uniqueness_sum / len(motif_lst)}, \"\n",
    "          f\"quality_avg: {quality_sum / len(motif_lst)}, sa_avg: {sa_sum / len(motif_lst)}, \"\n",
    "          f\"qed_avg: {qed_sum / len(motif_lst)}, div_avg: {div_sum / len(motif_lst)}, dist_avg: {dist_sum / len(motif_lst)}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main_motif()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.797979797979798 Quality: 0.44 SA: 4.02953527212658 QED: 0.7706266673604354 diversity: 0.5665239052519188 distance: 0.8765397306797168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:18<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.5858585858585859 Quality: 0.29 SA: 3.692897223129867 QED: 0.5564129098905336 diversity: 0.4522680830612557 distance: 0.8796205112529146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:37<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.95 uniqueness: 0.9894736842105263 Quality: 0.2 SA: 4.2302249966876575 QED: 0.44244132125298463 diversity: 0.7321740009755279 distance: 0.9032678747022594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:49<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.96 uniqueness: 1.0 Quality: 0.29 SA: 3.909284414340761 QED: 0.5145598633832891 diversity: 0.7281164348814452 distance: 0.8851942291535593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.34 Quality: 0.22 SA: 3.3381867833441383 QED: 0.6315214271902195 diversity: 0.3094609441460307 distance: 0.9028072979327085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9897959183673469 Quality: 0.73 SA: 3.087199066975929 QED: 0.7380253427114603 diversity: 0.7324647781745027 distance: 0.9044391216283378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:33<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 1.0 Quality: 0.65 SA: 3.1311924616387086 QED: 0.6532572868479127 diversity: 0.7417959299918974 distance: 0.8796495182813083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.97 uniqueness: 0.865979381443299 Quality: 0.0 SA: 4.866521582973061 QED: 0.42469777810189996 diversity: 0.4459507276827063 distance: 0.8446740500871041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.49 Quality: 0.36 SA: 2.773157701975621 QED: 0.7017392861488146 diversity: 0.4689384616999663 distance: 0.8667595494526619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.95 uniqueness: 1.0 Quality: 0.13 SA: 3.931251396652935 QED: 0.3863054897263462 diversity: 0.8365118745502933 distance: 0.8980199061132468\n",
      "运行时间: 319.6354 秒\n",
      "valid_ratio_avg: 0.977, uniqueness_avg: 0.8059087367859556, quality_avg: 0.45062487481502655, sa_avg: 3.6989450899845253, qed_avg: 0.5819587372613896, div_avg: 0.6014205140415544, dist_avg: 0.8840971789283817\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from dataset import SmileDataset, SmileCollator\n",
    "from tokenizer import SmilesTokenizer\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "# from test_connet import reconstruct\n",
    "from fragment_utils import reconstruct\n",
    "from tqdm import tqdm\n",
    "from utils.train_utils import seed_all\n",
    "from tdc import Oracle\n",
    "\n",
    "\n",
    "def calculate_tanimoto_distance(fingerprint1, fingerprint2):\n",
    "    \"\"\"\n",
    "    计算两个指纹之间的 Tanimoto 距离。\n",
    "    \"\"\"\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fingerprint1, fingerprint2)\n",
    "\n",
    "def calculate_morgan_fingerprint(mol, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算分子的 Morgan 指纹。\n",
    "    Args:\n",
    "        mol: RDKit 分子对象。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        分子指纹，或者如果分子无效则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_diversity(molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子的多样性（平均成对 Tanimoto 距离）。\n",
    "    Args:\n",
    "        molecules: RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        多样性值。\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_molecules = []\n",
    "    for mol in molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            fingerprints.append(fp)\n",
    "            valid_molecules.append(mol)\n",
    "    if not fingerprints:\n",
    "        return 0.0  # 如果没有有效分子，返回 0.0\n",
    "    n = len(fingerprints)\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = calculate_tanimoto_distance(fingerprints[i], fingerprints[j])\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def calculate_distance(generated_molecules, original_molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子与原始分子之间的平均 Tanimoto 距离。\n",
    "    Args:\n",
    "        generated_molecules: 生成的 RDKit 分子对象的列表。\n",
    "        original_molecules: 原始 RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        平均距离值。\n",
    "    \"\"\"\n",
    "    generated_fingerprints = []\n",
    "    original_fingerprints = []\n",
    "    # 计算生成分子的指纹\n",
    "    for mol in generated_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            generated_fingerprints.append(fp)\n",
    "    # 计算原始分子的指纹\n",
    "    for mol in original_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            original_fingerprints.append(fp)\n",
    "    if not generated_fingerprints or not original_fingerprints:\n",
    "        return 0.0\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for gen_fp in generated_fingerprints:\n",
    "        for orig_fp in original_fingerprints:\n",
    "            distance = calculate_tanimoto_distance(gen_fp, orig_fp)\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "\n",
    "def cal_QED(smiles):\n",
    "    oracle = Oracle(name = 'QED')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_SA(smiles):\n",
    "    oracle = Oracle(name = 'SA')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_all(smiles):\n",
    "    results = {}\n",
    "    results['QED'] = cal_QED(smiles)\n",
    "    results['SA'] = cal_SA(smiles)\n",
    "    return results\n",
    "\n",
    "\n",
    "def Test(model, smiles, tokenizer, max_seq_len, temperature, top_k, stream, rp, num_samples, kv_cache, is_simulation,\n",
    "         device, scaffold=False, linker=False):\n",
    "    complete_answer_list = []\n",
    "    valid_answer_list = []\n",
    "    model.eval()\n",
    "    # place data on the correct device\n",
    "    src_smiles = tokenizer.bos_token + smiles\n",
    "    x = torch.tensor(tokenizer.encode(src_smiles, add_special_tokens=False), dtype=torch.long).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        res_y = model.generate(x, tokenizer, max_new_tokens=max_seq_len,\n",
    "                               temperature=temperature, top_k=top_k, stream=stream, rp=rp, kv_cache=kv_cache,\n",
    "                               is_simulation=is_simulation)\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "\n",
    "        history_idx = 0\n",
    "        complete_answer = f\"{tokenizer.decode(x[0])}\"  # 用于保存整个生成的句子\n",
    "\n",
    "        while y != None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "            if not len(answer):\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 保存生成的片段到完整回答中\n",
    "            complete_answer += answer[history_idx:]\n",
    "\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "            if not stream:\n",
    "                break\n",
    "\n",
    "        complete_answer = complete_answer.replace(\" \", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "        frag_list = complete_answer.replace(\" \", \"\").split('[SEP]')\n",
    "        try:\n",
    "            if linker:\n",
    "                last_frag = frag_list[0].split('.')[1]\n",
    "                first_frag = frag_list[0].split('.')[0]\n",
    "                frag_list[0] = first_frag\n",
    "                frag_list[len(frag_list) - 1] = last_frag\n",
    "            frag_mol = [Chem.MolFromSmiles(s) for s in frag_list]\n",
    "            mol = reconstruct(frag_mol)[0]\n",
    "            if type(mol) == list:\n",
    "                mol = mol[0]\n",
    "            if mol:\n",
    "                generate_smiles = Chem.MolToSmiles(mol)\n",
    "                valid_answer_list.append(generate_smiles)\n",
    "                answer = frag_list\n",
    "            else:\n",
    "                answer = frag_list\n",
    "        except:\n",
    "            answer = frag_list\n",
    "        complete_answer_list.append(answer)\n",
    "\n",
    "    return complete_answer_list, valid_answer_list\n",
    "\n",
    "def main_motif():\n",
    "    motif_lst = ['*N1CC2(C[C@H]1C(=O)O)SCCS2[SEP]', '*C1Nc2cc(Cl)c(S(N)(=O)=O)cc2S(=O)(=O)N1[SEP]',\n",
    "     '*[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O[SEP]', '*[C@H]1CCN(C(=O)C=C)C1[SEP]', '*C1(CC#N)CN(S(=O)(=O)CC)C1[SEP]',\n",
    "     '*c1ccc2c(c1)OCCO2[SEP]', '*C[C@H](N)C(=O)O[SEP]',\n",
    "     '*[C@@H]1[C@@H]2C(=C[C@H](C)C[C@@H]2OC(=O)[C@@H](C)CC)C=C[C@@H]1C[SEP]', '*n1c(Br)nnc1SCC(=O)O[SEP]', '*OCCOC[SEP]']\n",
    "    original_smiles = ['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CC2(C[C@H]1C(=O)O)SCCS2',\n",
    "     'NS(=O)(=O)c1cc2c(cc1Cl)NC(C1CC3C=CC1C3)NS2(=O)=O', 'CC(C)Nc1nc2cc(Cl)c(Cl)cc2n1[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O',\n",
    "     'C=CC(=O)N1CC[C@H](n2nc(C#Cc3cc(OC)cc(OC)c3)c3c(N)ncnc32)C1',\n",
    "     'CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1', 'CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1)OCCO2',\n",
    "     'N[C@@H](Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1)C(=O)O',\n",
    "     'CC[C@H](C)C(=O)O[C@H]1C[C@@H](C)C=C2C=C[C@H](C)[C@H](CC[C@@H]3C[C@@H](O)CC(=O)O3)[C@H]21',\n",
    "     'O=C(O)CSc1nnc(Br)n1-c1ccc(C2CC2)c2ccccc12', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1']\n",
    "\n",
    "    # 设置随机种子的值\n",
    "    seed_value = 44\n",
    "    seed_all(seed_value)\n",
    "    # device = torch.device(f'cuda:{0}')  # 逻辑编号 cuda:0 对应 os.environ[\"CUDA_VISIBLE_DEVICES\"]中的第一个gpu\n",
    "    device = 'cuda:8'\n",
    "    batch_size = 1\n",
    "\n",
    "    test_names = \"test\"\n",
    "\n",
    "    tokenizer = SmilesTokenizer('./vocabs/vocab.txt')\n",
    "    tokenizer.bos_token = \"[BOS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"[BOS]\")\n",
    "    tokenizer.eos_token = \"[EOS]\"\n",
    "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
    "\n",
    "    mconf = GPTConfig(vocab_size=tokenizer.vocab_size, n_layer=12, n_head=12, n_embd=768)\n",
    "    model = GPT(mconf).to(device)\n",
    "    checkpoint = torch.load(f'./weights/fragpt.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    start_time = time.time()\n",
    "    valid_ratio_sum = 0\n",
    "    uniqueness_sum = 0\n",
    "    quality_sum = 0\n",
    "    sa_sum = 0\n",
    "    qed_sum = 0\n",
    "    div_sum = 0\n",
    "    dist_sum = 0\n",
    "    for i in motif_lst:\n",
    "        complete_answer_list, valid_answer_list = [], []\n",
    "        for j in tqdm(range(100)):\n",
    "            l1, l2 = Test(model, i, tokenizer, max_seq_len=512, temperature=1.2, top_k=8, stream=False, rp=1., num_samples=1,\n",
    "                 kv_cache=True, is_simulation=True, device=device)\n",
    "            if (len(l2) != 0):\n",
    "                valid_answer_list.append(l2[0])\n",
    "            if (len(l1) != 0):\n",
    "                complete_answer_list.append(l1[0])\n",
    "        unique_smiles = set(smile for smile in valid_answer_list if smile is not None)\n",
    "        unique_smiles_lst = list(unique_smiles)\n",
    "        num_unique_molecules = len(unique_smiles)\n",
    "        uniqueness = num_unique_molecules / len(valid_answer_list)\n",
    "        valid_ratio = len(valid_answer_list) / 100\n",
    "        results = cal_all(unique_smiles_lst)\n",
    "        SA_score = 0\n",
    "        QED_score = 0\n",
    "        sum = 0\n",
    "        for k in range(len(unique_smiles_lst)):\n",
    "            SA_score += results['SA'][k]\n",
    "            QED_score += results['QED'][k]\n",
    "            if (results['QED'][k] >= 0.6 and results['SA'][k] <= 4):\n",
    "                sum += 1\n",
    "\n",
    "        generated_molecules = [Chem.MolFromSmiles(s) for s in valid_answer_list]\n",
    "        original_molecules = [Chem.MolFromSmiles(s) for s in original_smiles]\n",
    "        # 计算多样性\n",
    "        diversity = calculate_diversity(generated_molecules)\n",
    "        # 计算距离\n",
    "        distance = calculate_distance(generated_molecules, original_molecules)\n",
    "\n",
    "        print('valid_ratio:', valid_ratio, 'uniqueness:', uniqueness, 'Quality:', sum / 100, 'SA:',\n",
    "              SA_score / len(unique_smiles_lst), 'QED:', QED_score / len(unique_smiles_lst), 'diversity:', diversity,\n",
    "              'distance:', distance)\n",
    "        valid_ratio_sum += valid_ratio\n",
    "        uniqueness_sum += uniqueness\n",
    "        quality_sum += sum / len(unique_smiles_lst)\n",
    "        sa_sum += SA_score / len(unique_smiles_lst)\n",
    "        qed_sum += QED_score / len(unique_smiles_lst)\n",
    "        div_sum += diversity\n",
    "        dist_sum += distance\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"运行时间: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"valid_ratio_avg: {valid_ratio_sum / len(motif_lst)}, uniqueness_avg: {uniqueness_sum / len(motif_lst)}, \"\n",
    "          f\"quality_avg: {quality_sum / len(motif_lst)}, sa_avg: {sa_sum / len(motif_lst)}, \"\n",
    "          f\"qed_avg: {qed_sum / len(motif_lst)}, div_avg: {div_sum / len(motif_lst)}, dist_avg: {dist_sum / len(motif_lst)}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main_motif()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b6fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fragpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
