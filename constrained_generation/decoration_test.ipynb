{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fcb6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d79cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:55<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9591836734693877 Quality: 0.0 SA: 4.648746773776923 QED: 0.29940560423427065 diversity: 0.514774788171485 distance: 0.8602808224300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:41<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.8181818181818182 Quality: 0.0 SA: 5.2379626814704725 QED: 0.5509248262224115 diversity: 0.43249001510840585 distance: 0.8769670605079892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:52<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.96 uniqueness: 1.0 Quality: 0.0 SA: 4.767620231407032 QED: 0.21389832864980288 diversity: 0.7214512037822068 distance: 0.8880674676881031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 1.0 Quality: 0.0 SA: 4.087600356829307 QED: 0.3195979216412912 diversity: 0.5898171599034514 distance: 0.8811540487817547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:43<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9285714285714286 Quality: 0.33 SA: 3.639270977129107 QED: 0.5236657096212078 diversity: 0.5041087956293959 distance: 0.8610817182846368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:36<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.97 uniqueness: 0.7319587628865979 Quality: 0.52 SA: 3.398029319579984 QED: 0.7278736244151535 diversity: 0.4594137986476902 distance: 0.877056829471974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:01<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.96 uniqueness: 0.9895833333333334 Quality: 0.12 SA: 3.434399139194173 QED: 0.3359712800467264 diversity: 0.7972910885720901 distance: 0.8999955380950962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 1.0 Quality: 0.0 SA: 5.04158227720461 QED: 0.3531797712074467 diversity: 0.6309185915343847 distance: 0.883350253599778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:45<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.8469387755102041 Quality: 0.29 SA: 2.9472681524333195 QED: 0.5426778383009112 diversity: 0.5300506057935626 distance: 0.8889104311701685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:02<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 1.0 Quality: 0.14 SA: 2.861856037053513 QED: 0.4156675579635617 diversity: 0.6408124649395137 distance: 0.8784294952156271\n",
      "运行时间: 747.0612 秒\n",
      "valid_ratio_avg: 0.9810000000000001, uniqueness_avg: 0.9274417791952769, quality_avg: 0.13999999999999999, sa_avg: 4.006433594607843, qed_avg: 0.42828624623027833, div_avg: 0.5821128512082188, dist_avg: 0.8795293665245157\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.train_utils import seed_all\n",
    "import argparse\n",
    "from tokenizer import SmilesTokenizer\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "from fragment_utils import reconstruct, reconstruct_d, reconstruct_scaffold1\n",
    "from torch.nn import functional as F\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def Test1(model, smiles, tokenizer, max_seq_len, temperature, top_k, stream, rp, num_samples, kv_cache, is_simulation,\n",
    "         device, scaffold=False, linker=False, dummy_lst=None):\n",
    "    complete_answer_list = []\n",
    "    valid_answer_list = []\n",
    "    model.eval()\n",
    "    # place data on the correct device\n",
    "    src_smiles = tokenizer.bos_token + smiles\n",
    "    x = torch.tensor(tokenizer.encode(src_smiles, add_special_tokens=False), dtype=torch.long).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        res_y = model.generate(x, tokenizer, max_new_tokens=max_seq_len,\n",
    "                               temperature=temperature, top_k=top_k, stream=stream, rp=rp, kv_cache=kv_cache,\n",
    "                               is_simulation=is_simulation)\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "\n",
    "        history_idx = 0\n",
    "        complete_answer = f\"{tokenizer.decode(x[0])}\"  # 用于保存整个生成的句子\n",
    "\n",
    "        while y != None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "            if not len(answer):\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 保存生成的片段到完整回答中\n",
    "            complete_answer += answer[history_idx:]\n",
    "\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "            if not stream:\n",
    "                break\n",
    "\n",
    "        complete_answer = complete_answer.replace(\" \", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "        frag_list = complete_answer.replace(\" \", \"\").split('[SEP]')\n",
    "        try:\n",
    "            if linker:\n",
    "                last_frag = frag_list[0].split('.')[1]\n",
    "                first_frag = frag_list[0].split('.')[0]\n",
    "                frag_list[0] = first_frag\n",
    "                frag_list[len(frag_list) - 1] = last_frag\n",
    "            frag_mol = [Chem.MolFromSmiles(s) for s in frag_list]\n",
    "            # frag_mol[0] = change_H2star(frag_list[0], dummy_lst)\n",
    "            mol = reconstruct_scaffold1(frag_mol, scaffold=scaffold)[0]\n",
    "            if type(mol) == list:\n",
    "                mol = mol[0]\n",
    "            if mol:\n",
    "                generate_smiles = Chem.MolToSmiles(mol)\n",
    "                valid_answer_list.append(generate_smiles)\n",
    "                answer = frag_list\n",
    "            else:\n",
    "                answer = frag_list\n",
    "        except:\n",
    "            answer = frag_list\n",
    "        complete_answer_list.append(answer)\n",
    "\n",
    "    return complete_answer_list, valid_answer_list\n",
    "\n",
    "def cal_QED(smiles):\n",
    "    oracle = Oracle(name = 'QED')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_SA(smiles):\n",
    "    oracle = Oracle(name = 'SA')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_all(smiles):\n",
    "    results = {}\n",
    "    results['QED'] = cal_QED(smiles)\n",
    "    results['SA'] = cal_SA(smiles)\n",
    "    return results\n",
    "\n",
    "def calculate_tanimoto_distance(fingerprint1, fingerprint2):\n",
    "    \"\"\"\n",
    "    计算两个指纹之间的 Tanimoto 距离。\n",
    "    \"\"\"\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fingerprint1, fingerprint2)\n",
    "\n",
    "def calculate_morgan_fingerprint(mol, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算分子的 Morgan 指纹。\n",
    "    Args:\n",
    "        mol: RDKit 分子对象。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        分子指纹，或者如果分子无效则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_diversity(molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子的多样性（平均成对 Tanimoto 距离）。\n",
    "    Args:\n",
    "        molecules: RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        多样性值。\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_molecules = []\n",
    "    for mol in molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            fingerprints.append(fp)\n",
    "            valid_molecules.append(mol)\n",
    "    if not fingerprints:\n",
    "        return 0.0  # 如果没有有效分子，返回 0.0\n",
    "    n = len(fingerprints)\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = calculate_tanimoto_distance(fingerprints[i], fingerprints[j])\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def calculate_distance(generated_molecules, original_molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子与原始分子之间的平均 Tanimoto 距离。\n",
    "    Args:\n",
    "        generated_molecules: 生成的 RDKit 分子对象的列表。\n",
    "        original_molecules: 原始 RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        平均距离值。\n",
    "    \"\"\"\n",
    "    generated_fingerprints = []\n",
    "    original_fingerprints = []\n",
    "    # 计算生成分子的指纹\n",
    "    for mol in generated_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            generated_fingerprints.append(fp)\n",
    "    # 计算原始分子的指纹\n",
    "    for mol in original_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            original_fingerprints.append(fp)\n",
    "    if not generated_fingerprints or not original_fingerprints:\n",
    "        return 0.0\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for gen_fp in generated_fingerprints:\n",
    "        for orig_fp in original_fingerprints:\n",
    "            distance = calculate_tanimoto_distance(gen_fp, orig_fp)\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def main_scaffold1():\n",
    "    scaffold_lst = ['*[C@H](CCc1ccccc1)N[C@@H](*)C(=O)N1CC2(C[C@H]1*)SCCS2[SEP]', '*c1cc2c(cc1*)NC(C1CC3C=CC1C3)NS2(=O)=O[SEP]',\n",
    "                    '*c1nc2cc(*)c(*)cc2n1[C@H]1O[C@@H](*)[C@H](*)[C@@H]1*[SEP]', '*N1CC[C@H](n2nc(C#Cc3cc(*)cc(*)c3)c3c(*)ncnc32)C1[SEP]',\n",
    "                    '*N1CC(*)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1[SEP]', '*[C@H](CN1CCCC1)[C@H](*)c1ccc2c(c1)OCCO2[SEP]',\n",
    "                    '*c1cc(*)c(Oc2ccc(*)c(*)c2)c(*)c1[SEP]', '*[C@H]1C[C@@H](*)C=C2C=C[C@H](*)[C@H](CC[C@@H]3C[C@@H](*)CC(=O)O3)[C@H]21[SEP]',\n",
    "                    '*c1nnc(*)n1-c1ccc(C2CC2)c2ccccc12[SEP]', '*c1cccc(Nc2ncnc3cc(*)c(*)cc23)c1[SEP]']\n",
    "    original_smiles = ['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CC2(C[C@H]1C(=O)O)SCCS2',\n",
    "                       'NS(=O)(=O)c1cc2c(cc1Cl)NC(C1CC3C=CC1C3)NS2(=O)=O',\n",
    "                       'CC(C)Nc1nc2cc(Cl)c(Cl)cc2n1[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O',\n",
    "                       'C=CC(=O)N1CC[C@H](n2nc(C#Cc3cc(OC)cc(OC)c3)c3c(N)ncnc32)C1',\n",
    "                       'CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1',\n",
    "                       'CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1)OCCO2',\n",
    "                       'N[C@@H](Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1)C(=O)O',\n",
    "                       'CC[C@H](C)C(=O)O[C@H]1C[C@@H](C)C=C2C=C[C@H](C)[C@H](CC[C@@H]3C[C@@H](O)CC(=O)O3)[C@H]21',\n",
    "                       'O=C(O)CSc1nnc(Br)n1-c1ccc(C2CC2)c2ccccc12', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1']\n",
    "\n",
    "    # 设置随机种子的值\n",
    "    seed_value = 42\n",
    "    seed_all(seed_value)\n",
    "    # device = torch.device(f'cuda:{0}')  # 逻辑编号 cuda:0 对应 os.environ[\"CUDA_VISIBLE_DEVICES\"]中的第一个gpu\n",
    "    device = 'cuda:1'\n",
    "    batch_size = 1\n",
    "\n",
    "    test_names = \"test\"\n",
    "\n",
    "    tokenizer = SmilesTokenizer('./vocabs/vocab.txt')\n",
    "    tokenizer.bos_token = \"[BOS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"[BOS]\")\n",
    "    tokenizer.eos_token = \"[EOS]\"\n",
    "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
    "\n",
    "    mconf = GPTConfig(vocab_size=tokenizer.vocab_size, n_layer=12, n_head=12, n_embd=768)\n",
    "    model = GPT(mconf).to(device)\n",
    "    checkpoint = torch.load(f'./weights/fragpt.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    start_time = time.time()\n",
    "    valid_ratio_sum = 0\n",
    "    uniqueness_sum = 0\n",
    "    quality_sum = 0\n",
    "    sa_sum = 0\n",
    "    qed_sum = 0\n",
    "    div_sum = 0\n",
    "    dist_sum = 0\n",
    "    for i in scaffold_lst:\n",
    "        complete_answer_list, valid_answer_list = [], []\n",
    "        for j in tqdm(range(100)):\n",
    "            num_stars = i.count('*')\n",
    "            l1 = []\n",
    "            l2 = []\n",
    "            scaf = i\n",
    "            for k in range(num_stars):\n",
    "                l1, l2 = Test1(model, scaf, tokenizer, max_seq_len=512, temperature=1.2, top_k=8, stream=False, rp=1.,\n",
    "                              num_samples=1, kv_cache=True, is_simulation=True, device=device, scaffold=True, dummy_lst=None)\n",
    "                if (len(l2) != 0):\n",
    "                    scaf = l2[0] + '[SEP]'\n",
    "                else:\n",
    "                    l2 = []\n",
    "                    break\n",
    "\n",
    "            if (len(l2) != 0):\n",
    "                valid_answer_list.append(l2[0])\n",
    "            if (len(l1) != 0):\n",
    "                complete_answer_list.append(l1[0])\n",
    "        unique_smiles = set(smile for smile in valid_answer_list if smile is not None)\n",
    "        unique_smiles_lst = list(unique_smiles)\n",
    "        num_unique_molecules = len(unique_smiles)\n",
    "        if len(valid_answer_list) == 0:\n",
    "            uniqueness = 0\n",
    "        else:\n",
    "            uniqueness = num_unique_molecules / len(valid_answer_list)\n",
    "        valid_ratio = len(valid_answer_list) / 100\n",
    "        results = cal_all(unique_smiles_lst)\n",
    "        SA_score = 0\n",
    "        QED_score = 0\n",
    "        sum = 0\n",
    "        for i in range(len(unique_smiles_lst)):\n",
    "            SA_score += results['SA'][i]\n",
    "            QED_score += results['QED'][i]\n",
    "            if (results['QED'][i] >= 0.6 and results['SA'][i] <= 4):\n",
    "                sum += 1\n",
    "\n",
    "        generated_molecules = [Chem.MolFromSmiles(s) for s in valid_answer_list]\n",
    "        original_molecules = [Chem.MolFromSmiles(s) for s in original_smiles]\n",
    "        # 计算多样性\n",
    "        diversity = calculate_diversity(generated_molecules)\n",
    "        # 计算距离\n",
    "        distance = calculate_distance(generated_molecules, original_molecules)\n",
    "\n",
    "        print('valid_ratio:', valid_ratio, 'uniqueness:', uniqueness, 'Quality:', sum / 100,\n",
    "              'SA:', SA_score / len(unique_smiles_lst) if len(unique_smiles_lst) != 0 else 0,\n",
    "              'QED:', QED_score / len(unique_smiles_lst) if len(unique_smiles_lst) != 0 else 0, 'diversity:', diversity,\n",
    "              'distance:', distance)\n",
    "        valid_ratio_sum += valid_ratio\n",
    "        uniqueness_sum += uniqueness\n",
    "        quality_sum += sum / 100\n",
    "        if len(unique_smiles_lst) == 0:\n",
    "            sa_sum += 0\n",
    "            qed_sum += 0\n",
    "        else:\n",
    "            sa_sum += SA_score / len(unique_smiles_lst)\n",
    "            qed_sum += QED_score / len(unique_smiles_lst)\n",
    "        div_sum += diversity\n",
    "        dist_sum += distance\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"运行时间: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"valid_ratio_avg: {valid_ratio_sum / len(scaffold_lst)}, uniqueness_avg: {uniqueness_sum / len(scaffold_lst)}, \"\n",
    "          f\"quality_avg: {quality_sum / len(scaffold_lst)}, sa_avg: {sa_sum / len(scaffold_lst)}, \"\n",
    "          f\"qed_avg: {qed_sum / len(scaffold_lst)}, div_avg: {div_sum / len(scaffold_lst)}, dist_avg: {dist_sum / len(scaffold_lst)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main_scaffold1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.9191919191919192 Quality: 0.0 SA: 4.697212258575822 QED: 0.30147140309002535 diversity: 0.5027161298723802 distance: 0.861385941711051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:41<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.8181818181818182 Quality: 0.0 SA: 5.276289055956925 QED: 0.49709941948210257 diversity: 0.4584105458611867 distance: 0.8755641327813003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.96 uniqueness: 1.0 Quality: 0.0 SA: 4.68127316094573 QED: 0.22205383593845462 diversity: 0.7137937328164814 distance: 0.8881307980943028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.98989898989899 Quality: 0.0 SA: 4.100626608168272 QED: 0.3066893636285204 diversity: 0.5981227553823617 distance: 0.8814898183463524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:45<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9387755102040817 Quality: 0.38 SA: 3.512168295250528 QED: 0.5701544171921638 diversity: 0.5028691971315462 distance: 0.8615018650938778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:39<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.66 Quality: 0.43 SA: 3.4269124835747427 QED: 0.703307622975149 diversity: 0.4544083566326828 distance: 0.8778992326206647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:45<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.96 uniqueness: 1.0 Quality: 0.08 SA: 3.5124921377734624 QED: 0.30838078361518945 diversity: 0.7996214648139455 distance: 0.8993325423006705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 1.0 Quality: 0.0 SA: 5.043124845188668 QED: 0.36141096257281163 diversity: 0.6247990655175163 distance: 0.8826296102352351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:44<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.8282828282828283 Quality: 0.3 SA: 2.8616321139229757 QED: 0.5385594254146534 diversity: 0.5360562266119304 distance: 0.88835710681864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.98989898989899 Quality: 0.12 SA: 2.8862842142744167 QED: 0.434221124523414 diversity: 0.6323198571269547 distance: 0.8776725835055529\n",
      "运行时间: 737.8209 秒\n",
      "valid_ratio_avg: 0.983, uniqueness_avg: 0.9144230055658626, quality_avg: 0.131, sa_avg: 3.9998015173631543, qed_avg: 0.4243348358432484, div_avg: 0.5823117331766986, dist_avg: 0.879396363150765\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.train_utils import seed_all\n",
    "import argparse\n",
    "from tokenizer import SmilesTokenizer\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "from fragment_utils import reconstruct, reconstruct_d, reconstruct_scaffold1\n",
    "from torch.nn import functional as F\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def Test1(model, smiles, tokenizer, max_seq_len, temperature, top_k, stream, rp, num_samples, kv_cache, is_simulation,\n",
    "         device, scaffold=False, linker=False, dummy_lst=None):\n",
    "    complete_answer_list = []\n",
    "    valid_answer_list = []\n",
    "    model.eval()\n",
    "    # place data on the correct device\n",
    "    src_smiles = tokenizer.bos_token + smiles\n",
    "    x = torch.tensor(tokenizer.encode(src_smiles, add_special_tokens=False), dtype=torch.long).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        res_y = model.generate(x, tokenizer, max_new_tokens=max_seq_len,\n",
    "                               temperature=temperature, top_k=top_k, stream=stream, rp=rp, kv_cache=kv_cache,\n",
    "                               is_simulation=is_simulation)\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "\n",
    "        history_idx = 0\n",
    "        complete_answer = f\"{tokenizer.decode(x[0])}\"  # 用于保存整个生成的句子\n",
    "\n",
    "        while y != None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "            if not len(answer):\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 保存生成的片段到完整回答中\n",
    "            complete_answer += answer[history_idx:]\n",
    "\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "            if not stream:\n",
    "                break\n",
    "\n",
    "        complete_answer = complete_answer.replace(\" \", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "        frag_list = complete_answer.replace(\" \", \"\").split('[SEP]')\n",
    "        try:\n",
    "            if linker:\n",
    "                last_frag = frag_list[0].split('.')[1]\n",
    "                first_frag = frag_list[0].split('.')[0]\n",
    "                frag_list[0] = first_frag\n",
    "                frag_list[len(frag_list) - 1] = last_frag\n",
    "            frag_mol = [Chem.MolFromSmiles(s) for s in frag_list]\n",
    "            # frag_mol[0] = change_H2star(frag_list[0], dummy_lst)\n",
    "            mol = reconstruct_scaffold1(frag_mol, scaffold=scaffold)[0]\n",
    "            if type(mol) == list:\n",
    "                mol = mol[0]\n",
    "            if mol:\n",
    "                generate_smiles = Chem.MolToSmiles(mol)\n",
    "                valid_answer_list.append(generate_smiles)\n",
    "                answer = frag_list\n",
    "            else:\n",
    "                answer = frag_list\n",
    "        except:\n",
    "            answer = frag_list\n",
    "        complete_answer_list.append(answer)\n",
    "\n",
    "    return complete_answer_list, valid_answer_list\n",
    "\n",
    "def cal_QED(smiles):\n",
    "    oracle = Oracle(name = 'QED')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_SA(smiles):\n",
    "    oracle = Oracle(name = 'SA')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_all(smiles):\n",
    "    results = {}\n",
    "    results['QED'] = cal_QED(smiles)\n",
    "    results['SA'] = cal_SA(smiles)\n",
    "    return results\n",
    "\n",
    "def calculate_tanimoto_distance(fingerprint1, fingerprint2):\n",
    "    \"\"\"\n",
    "    计算两个指纹之间的 Tanimoto 距离。\n",
    "    \"\"\"\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fingerprint1, fingerprint2)\n",
    "\n",
    "def calculate_morgan_fingerprint(mol, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算分子的 Morgan 指纹。\n",
    "    Args:\n",
    "        mol: RDKit 分子对象。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        分子指纹，或者如果分子无效则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_diversity(molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子的多样性（平均成对 Tanimoto 距离）。\n",
    "    Args:\n",
    "        molecules: RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        多样性值。\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_molecules = []\n",
    "    for mol in molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            fingerprints.append(fp)\n",
    "            valid_molecules.append(mol)\n",
    "    if not fingerprints:\n",
    "        return 0.0  # 如果没有有效分子，返回 0.0\n",
    "    n = len(fingerprints)\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = calculate_tanimoto_distance(fingerprints[i], fingerprints[j])\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def calculate_distance(generated_molecules, original_molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子与原始分子之间的平均 Tanimoto 距离。\n",
    "    Args:\n",
    "        generated_molecules: 生成的 RDKit 分子对象的列表。\n",
    "        original_molecules: 原始 RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        平均距离值。\n",
    "    \"\"\"\n",
    "    generated_fingerprints = []\n",
    "    original_fingerprints = []\n",
    "    # 计算生成分子的指纹\n",
    "    for mol in generated_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            generated_fingerprints.append(fp)\n",
    "    # 计算原始分子的指纹\n",
    "    for mol in original_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            original_fingerprints.append(fp)\n",
    "    if not generated_fingerprints or not original_fingerprints:\n",
    "        return 0.0\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for gen_fp in generated_fingerprints:\n",
    "        for orig_fp in original_fingerprints:\n",
    "            distance = calculate_tanimoto_distance(gen_fp, orig_fp)\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def main_scaffold1():\n",
    "    scaffold_lst = ['*[C@H](CCc1ccccc1)N[C@@H](*)C(=O)N1CC2(C[C@H]1*)SCCS2[SEP]', '*c1cc2c(cc1*)NC(C1CC3C=CC1C3)NS2(=O)=O[SEP]',\n",
    "                    '*c1nc2cc(*)c(*)cc2n1[C@H]1O[C@@H](*)[C@H](*)[C@@H]1*[SEP]', '*N1CC[C@H](n2nc(C#Cc3cc(*)cc(*)c3)c3c(*)ncnc32)C1[SEP]',\n",
    "                    '*N1CC(*)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1[SEP]', '*[C@H](CN1CCCC1)[C@H](*)c1ccc2c(c1)OCCO2[SEP]',\n",
    "                    '*c1cc(*)c(Oc2ccc(*)c(*)c2)c(*)c1[SEP]', '*[C@H]1C[C@@H](*)C=C2C=C[C@H](*)[C@H](CC[C@@H]3C[C@@H](*)CC(=O)O3)[C@H]21[SEP]',\n",
    "                    '*c1nnc(*)n1-c1ccc(C2CC2)c2ccccc12[SEP]', '*c1cccc(Nc2ncnc3cc(*)c(*)cc23)c1[SEP]']\n",
    "    original_smiles = ['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CC2(C[C@H]1C(=O)O)SCCS2',\n",
    "                       'NS(=O)(=O)c1cc2c(cc1Cl)NC(C1CC3C=CC1C3)NS2(=O)=O',\n",
    "                       'CC(C)Nc1nc2cc(Cl)c(Cl)cc2n1[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O',\n",
    "                       'C=CC(=O)N1CC[C@H](n2nc(C#Cc3cc(OC)cc(OC)c3)c3c(N)ncnc32)C1',\n",
    "                       'CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1',\n",
    "                       'CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1)OCCO2',\n",
    "                       'N[C@@H](Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1)C(=O)O',\n",
    "                       'CC[C@H](C)C(=O)O[C@H]1C[C@@H](C)C=C2C=C[C@H](C)[C@H](CC[C@@H]3C[C@@H](O)CC(=O)O3)[C@H]21',\n",
    "                       'O=C(O)CSc1nnc(Br)n1-c1ccc(C2CC2)c2ccccc12', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1']\n",
    "\n",
    "    # 设置随机种子的值\n",
    "    seed_value = 43\n",
    "    seed_all(seed_value)\n",
    "    # device = torch.device(f'cuda:{0}')  # 逻辑编号 cuda:0 对应 os.environ[\"CUDA_VISIBLE_DEVICES\"]中的第一个gpu\n",
    "    device = 'cuda:1'\n",
    "    batch_size = 1\n",
    "\n",
    "    test_names = \"test\"\n",
    "\n",
    "    tokenizer = SmilesTokenizer('./vocabs/vocab.txt')\n",
    "    tokenizer.bos_token = \"[BOS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"[BOS]\")\n",
    "    tokenizer.eos_token = \"[EOS]\"\n",
    "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
    "\n",
    "    mconf = GPTConfig(vocab_size=tokenizer.vocab_size, n_layer=12, n_head=12, n_embd=768)\n",
    "    model = GPT(mconf).to(device)\n",
    "    checkpoint = torch.load(f'./weights/fragpt.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    start_time = time.time()\n",
    "    valid_ratio_sum = 0\n",
    "    uniqueness_sum = 0\n",
    "    quality_sum = 0\n",
    "    sa_sum = 0\n",
    "    qed_sum = 0\n",
    "    div_sum = 0\n",
    "    dist_sum = 0\n",
    "    for i in scaffold_lst:\n",
    "        complete_answer_list, valid_answer_list = [], []\n",
    "        for j in tqdm(range(100)):\n",
    "            num_stars = i.count('*')\n",
    "            l1 = []\n",
    "            l2 = []\n",
    "            scaf = i\n",
    "            for k in range(num_stars):\n",
    "                l1, l2 = Test1(model, scaf, tokenizer, max_seq_len=512, temperature=1.2, top_k=8, stream=False, rp=1.,\n",
    "                              num_samples=1, kv_cache=True, is_simulation=True, device=device, scaffold=True, dummy_lst=None)\n",
    "                if (len(l2) != 0):\n",
    "                    scaf = l2[0] + '[SEP]'\n",
    "                else:\n",
    "                    l2 = []\n",
    "                    break\n",
    "\n",
    "            if (len(l2) != 0):\n",
    "                valid_answer_list.append(l2[0])\n",
    "            if (len(l1) != 0):\n",
    "                complete_answer_list.append(l1[0])\n",
    "        unique_smiles = set(smile for smile in valid_answer_list if smile is not None)\n",
    "        unique_smiles_lst = list(unique_smiles)\n",
    "        num_unique_molecules = len(unique_smiles)\n",
    "        if len(valid_answer_list) == 0:\n",
    "            uniqueness = 0\n",
    "        else:\n",
    "            uniqueness = num_unique_molecules / len(valid_answer_list)\n",
    "        valid_ratio = len(valid_answer_list) / 100\n",
    "        results = cal_all(unique_smiles_lst)\n",
    "        SA_score = 0\n",
    "        QED_score = 0\n",
    "        sum = 0\n",
    "        for i in range(len(unique_smiles_lst)):\n",
    "            SA_score += results['SA'][i]\n",
    "            QED_score += results['QED'][i]\n",
    "            if (results['QED'][i] >= 0.6 and results['SA'][i] <= 4):\n",
    "                sum += 1\n",
    "\n",
    "        generated_molecules = [Chem.MolFromSmiles(s) for s in valid_answer_list]\n",
    "        original_molecules = [Chem.MolFromSmiles(s) for s in original_smiles]\n",
    "        # 计算多样性\n",
    "        diversity = calculate_diversity(generated_molecules)\n",
    "        # 计算距离\n",
    "        distance = calculate_distance(generated_molecules, original_molecules)\n",
    "\n",
    "        print('valid_ratio:', valid_ratio, 'uniqueness:', uniqueness, 'Quality:', sum / 100,\n",
    "              'SA:', SA_score / len(unique_smiles_lst) if len(unique_smiles_lst) != 0 else 0,\n",
    "              'QED:', QED_score / len(unique_smiles_lst) if len(unique_smiles_lst) != 0 else 0, 'diversity:', diversity,\n",
    "              'distance:', distance)\n",
    "        valid_ratio_sum += valid_ratio\n",
    "        uniqueness_sum += uniqueness\n",
    "        quality_sum += sum / 100\n",
    "        if len(unique_smiles_lst) == 0:\n",
    "            sa_sum += 0\n",
    "            qed_sum += 0\n",
    "        else:\n",
    "            sa_sum += SA_score / len(unique_smiles_lst)\n",
    "            qed_sum += QED_score / len(unique_smiles_lst)\n",
    "        div_sum += diversity\n",
    "        dist_sum += distance\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"运行时间: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"valid_ratio_avg: {valid_ratio_sum / len(scaffold_lst)}, uniqueness_avg: {uniqueness_sum / len(scaffold_lst)}, \"\n",
    "          f\"quality_avg: {quality_sum / len(scaffold_lst)}, sa_avg: {sa_sum / len(scaffold_lst)}, \"\n",
    "          f\"qed_avg: {qed_sum / len(scaffold_lst)}, div_avg: {div_sum / len(scaffold_lst)}, dist_avg: {dist_sum / len(scaffold_lst)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main_scaffold1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:54<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.9292929292929293 Quality: 0.0 SA: 4.62511107780839 QED: 0.28365229612443277 diversity: 0.5030579755639942 distance: 0.860279389033996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:40<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.71 Quality: 0.0 SA: 5.249130694091 QED: 0.5183964208995867 diversity: 0.4191896967450992 distance: 0.874526174823528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:53<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.94 uniqueness: 1.0 Quality: 0.0 SA: 4.7014704158581155 QED: 0.250556277450295 diversity: 0.7111368123683309 distance: 0.8869435833624065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 1.0 Quality: 0.0 SA: 4.1032781024516405 QED: 0.3186953505670136 diversity: 0.5908785555298866 distance: 0.8788152637198187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:40<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.9090909090909091 Quality: 0.38 SA: 3.54228642711326 QED: 0.5782284351774114 diversity: 0.48354154651367887 distance: 0.8592795848663494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:43<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 1.0 uniqueness: 0.65 Quality: 0.42 SA: 3.558973110928189 QED: 0.6973995975483059 diversity: 0.4697017357247409 distance: 0.8776460473169019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:59<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.94 uniqueness: 1.0 Quality: 0.1 SA: 3.290568007773773 QED: 0.3519062625371475 diversity: 0.7822124141699802 distance: 0.8997856211672299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.98 uniqueness: 0.9897959183673469 Quality: 0.0 SA: 5.072694033264056 QED: 0.35038868617602437 diversity: 0.6283324285301455 distance: 0.8837861291561301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:49<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.8282828282828283 Quality: 0.21 SA: 3.1003452567778504 QED: 0.46866353035092967 diversity: 0.5603871323243632 distance: 0.8894257579077501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_ratio: 0.99 uniqueness: 0.9595959595959596 Quality: 0.05 SA: 2.973643491900483 QED: 0.38974669489326585 diversity: 0.6413424937428334 distance: 0.8810009024699741\n",
      "运行时间: 756.5582 秒\n",
      "valid_ratio_avg: 0.9810000000000001, uniqueness_avg: 0.8976058544629973, quality_avg: 0.11600000000000002, sa_avg: 4.021750061796675, qed_avg: 0.42076335517244134, div_avg: 0.5789780791213053, dist_avg: 0.8791488453824086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.train_utils import seed_all\n",
    "import argparse\n",
    "from tokenizer import SmilesTokenizer\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "from fragment_utils import reconstruct, reconstruct_d, reconstruct_scaffold1\n",
    "from torch.nn import functional as F\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def Test1(model, smiles, tokenizer, max_seq_len, temperature, top_k, stream, rp, num_samples, kv_cache, is_simulation,\n",
    "         device, scaffold=False, linker=False, dummy_lst=None):\n",
    "    complete_answer_list = []\n",
    "    valid_answer_list = []\n",
    "    model.eval()\n",
    "    # place data on the correct device\n",
    "    src_smiles = tokenizer.bos_token + smiles\n",
    "    x = torch.tensor(tokenizer.encode(src_smiles, add_special_tokens=False), dtype=torch.long).unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        res_y = model.generate(x, tokenizer, max_new_tokens=max_seq_len,\n",
    "                               temperature=temperature, top_k=top_k, stream=stream, rp=rp, kv_cache=kv_cache,\n",
    "                               is_simulation=is_simulation)\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "\n",
    "        history_idx = 0\n",
    "        complete_answer = f\"{tokenizer.decode(x[0])}\"  # 用于保存整个生成的句子\n",
    "\n",
    "        while y != None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "            if not len(answer):\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # 保存生成的片段到完整回答中\n",
    "            complete_answer += answer[history_idx:]\n",
    "\n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "            if not stream:\n",
    "                break\n",
    "\n",
    "        complete_answer = complete_answer.replace(\" \", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "        frag_list = complete_answer.replace(\" \", \"\").split('[SEP]')\n",
    "        try:\n",
    "            if linker:\n",
    "                last_frag = frag_list[0].split('.')[1]\n",
    "                first_frag = frag_list[0].split('.')[0]\n",
    "                frag_list[0] = first_frag\n",
    "                frag_list[len(frag_list) - 1] = last_frag\n",
    "            frag_mol = [Chem.MolFromSmiles(s) for s in frag_list]\n",
    "            # frag_mol[0] = change_H2star(frag_list[0], dummy_lst)\n",
    "            mol = reconstruct_scaffold1(frag_mol, scaffold=scaffold)[0]\n",
    "            if type(mol) == list:\n",
    "                mol = mol[0]\n",
    "            if mol:\n",
    "                generate_smiles = Chem.MolToSmiles(mol)\n",
    "                valid_answer_list.append(generate_smiles)\n",
    "                answer = frag_list\n",
    "            else:\n",
    "                answer = frag_list\n",
    "        except:\n",
    "            answer = frag_list\n",
    "        complete_answer_list.append(answer)\n",
    "\n",
    "    return complete_answer_list, valid_answer_list\n",
    "\n",
    "def cal_QED(smiles):\n",
    "    oracle = Oracle(name = 'QED')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_SA(smiles):\n",
    "    oracle = Oracle(name = 'SA')\n",
    "    return oracle(smiles)\n",
    "\n",
    "def cal_all(smiles):\n",
    "    results = {}\n",
    "    results['QED'] = cal_QED(smiles)\n",
    "    results['SA'] = cal_SA(smiles)\n",
    "    return results\n",
    "\n",
    "def calculate_tanimoto_distance(fingerprint1, fingerprint2):\n",
    "    \"\"\"\n",
    "    计算两个指纹之间的 Tanimoto 距离。\n",
    "    \"\"\"\n",
    "    return 1 - DataStructs.TanimotoSimilarity(fingerprint1, fingerprint2)\n",
    "\n",
    "def calculate_morgan_fingerprint(mol, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算分子的 Morgan 指纹。\n",
    "    Args:\n",
    "        mol: RDKit 分子对象。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        分子指纹，或者如果分子无效则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        return fp\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_diversity(molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子的多样性（平均成对 Tanimoto 距离）。\n",
    "    Args:\n",
    "        molecules: RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        多样性值。\n",
    "    \"\"\"\n",
    "    fingerprints = []\n",
    "    valid_molecules = []\n",
    "    for mol in molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            fingerprints.append(fp)\n",
    "            valid_molecules.append(mol)\n",
    "    if not fingerprints:\n",
    "        return 0.0  # 如果没有有效分子，返回 0.0\n",
    "    n = len(fingerprints)\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = calculate_tanimoto_distance(fingerprints[i], fingerprints[j])\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def calculate_distance(generated_molecules, original_molecules, radius=2, nBits=2048):\n",
    "    \"\"\"\n",
    "    计算生成分子与原始分子之间的平均 Tanimoto 距离。\n",
    "    Args:\n",
    "        generated_molecules: 生成的 RDKit 分子对象的列表。\n",
    "        original_molecules: 原始 RDKit 分子对象的列表。\n",
    "        radius: Morgan 指纹的半径。\n",
    "        nBits: 指纹的位数。\n",
    "    Returns:\n",
    "        平均距离值。\n",
    "    \"\"\"\n",
    "    generated_fingerprints = []\n",
    "    original_fingerprints = []\n",
    "    # 计算生成分子的指纹\n",
    "    for mol in generated_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            generated_fingerprints.append(fp)\n",
    "    # 计算原始分子的指纹\n",
    "    for mol in original_molecules:\n",
    "        fp = calculate_morgan_fingerprint(mol, radius, nBits)\n",
    "        if fp is not None:\n",
    "            original_fingerprints.append(fp)\n",
    "    if not generated_fingerprints or not original_fingerprints:\n",
    "        return 0.0\n",
    "    total_distance = 0.0\n",
    "    count = 0\n",
    "    for gen_fp in generated_fingerprints:\n",
    "        for orig_fp in original_fingerprints:\n",
    "            distance = calculate_tanimoto_distance(gen_fp, orig_fp)\n",
    "            total_distance += distance\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0.0\n",
    "    return total_distance / count\n",
    "\n",
    "def main_scaffold1():\n",
    "    scaffold_lst = ['*[C@H](CCc1ccccc1)N[C@@H](*)C(=O)N1CC2(C[C@H]1*)SCCS2[SEP]', '*c1cc2c(cc1*)NC(C1CC3C=CC1C3)NS2(=O)=O[SEP]',\n",
    "                    '*c1nc2cc(*)c(*)cc2n1[C@H]1O[C@@H](*)[C@H](*)[C@@H]1*[SEP]', '*N1CC[C@H](n2nc(C#Cc3cc(*)cc(*)c3)c3c(*)ncnc32)C1[SEP]',\n",
    "                    '*N1CC(*)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1[SEP]', '*[C@H](CN1CCCC1)[C@H](*)c1ccc2c(c1)OCCO2[SEP]',\n",
    "                    '*c1cc(*)c(Oc2ccc(*)c(*)c2)c(*)c1[SEP]', '*[C@H]1C[C@@H](*)C=C2C=C[C@H](*)[C@H](CC[C@@H]3C[C@@H](*)CC(=O)O3)[C@H]21[SEP]',\n",
    "                    '*c1nnc(*)n1-c1ccc(C2CC2)c2ccccc12[SEP]', '*c1cccc(Nc2ncnc3cc(*)c(*)cc23)c1[SEP]']\n",
    "    original_smiles = ['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N1CC2(C[C@H]1C(=O)O)SCCS2',\n",
    "                       'NS(=O)(=O)c1cc2c(cc1Cl)NC(C1CC3C=CC1C3)NS2(=O)=O',\n",
    "                       'CC(C)Nc1nc2cc(Cl)c(Cl)cc2n1[C@H]1O[C@@H](CO)[C@H](O)[C@@H]1O',\n",
    "                       'C=CC(=O)N1CC[C@H](n2nc(C#Cc3cc(OC)cc(OC)c3)c3c(N)ncnc32)C1',\n",
    "                       'CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)cn2)C1',\n",
    "                       'CCCCCCCC(=O)N[C@H](CN1CCCC1)[C@H](O)c1ccc2c(c1)OCCO2',\n",
    "                       'N[C@@H](Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1)C(=O)O',\n",
    "                       'CC[C@H](C)C(=O)O[C@H]1C[C@@H](C)C=C2C=C[C@H](C)[C@H](CC[C@@H]3C[C@@H](O)CC(=O)O3)[C@H]21',\n",
    "                       'O=C(O)CSc1nnc(Br)n1-c1ccc(C2CC2)c2ccccc12', 'C#Cc1cccc(Nc2ncnc3cc(OCCOC)c(OCCOC)cc23)c1']\n",
    "\n",
    "    # 设置随机种子的值\n",
    "    seed_value = 44\n",
    "    seed_all(seed_value)\n",
    "    # device = torch.device(f'cuda:{0}')  # 逻辑编号 cuda:0 对应 os.environ[\"CUDA_VISIBLE_DEVICES\"]中的第一个gpu\n",
    "    device = 'cuda:1'\n",
    "    batch_size = 1\n",
    "\n",
    "    test_names = \"test\"\n",
    "\n",
    "    tokenizer = SmilesTokenizer('./vocabs/vocab.txt')\n",
    "    tokenizer.bos_token = \"[BOS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids(\"[BOS]\")\n",
    "    tokenizer.eos_token = \"[EOS]\"\n",
    "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(\"[EOS]\")\n",
    "\n",
    "    mconf = GPTConfig(vocab_size=tokenizer.vocab_size, n_layer=12, n_head=12, n_embd=768)\n",
    "    model = GPT(mconf).to(device)\n",
    "    checkpoint = torch.load(f'./weights/fragpt.pt', weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    start_time = time.time()\n",
    "    valid_ratio_sum = 0\n",
    "    uniqueness_sum = 0\n",
    "    quality_sum = 0\n",
    "    sa_sum = 0\n",
    "    qed_sum = 0\n",
    "    div_sum = 0\n",
    "    dist_sum = 0\n",
    "    for i in scaffold_lst:\n",
    "        complete_answer_list, valid_answer_list = [], []\n",
    "        for j in tqdm(range(100)):\n",
    "            num_stars = i.count('*')\n",
    "            l1 = []\n",
    "            l2 = []\n",
    "            scaf = i\n",
    "            for k in range(num_stars):\n",
    "                l1, l2 = Test1(model, scaf, tokenizer, max_seq_len=512, temperature=1.2, top_k=8, stream=False, rp=1.,\n",
    "                              num_samples=1, kv_cache=True, is_simulation=True, device=device, scaffold=True, dummy_lst=None)\n",
    "                if (len(l2) != 0):\n",
    "                    scaf = l2[0] + '[SEP]'\n",
    "                else:\n",
    "                    l2 = []\n",
    "                    break\n",
    "\n",
    "            if (len(l2) != 0):\n",
    "                valid_answer_list.append(l2[0])\n",
    "            if (len(l1) != 0):\n",
    "                complete_answer_list.append(l1[0])\n",
    "        unique_smiles = set(smile for smile in valid_answer_list if smile is not None)\n",
    "        unique_smiles_lst = list(unique_smiles)\n",
    "        num_unique_molecules = len(unique_smiles)\n",
    "        if len(valid_answer_list) == 0:\n",
    "            uniqueness = 0\n",
    "        else:\n",
    "            uniqueness = num_unique_molecules / len(valid_answer_list)\n",
    "        valid_ratio = len(valid_answer_list) / 100\n",
    "        results = cal_all(unique_smiles_lst)\n",
    "        SA_score = 0\n",
    "        QED_score = 0\n",
    "        sum = 0\n",
    "        for i in range(len(unique_smiles_lst)):\n",
    "            SA_score += results['SA'][i]\n",
    "            QED_score += results['QED'][i]\n",
    "            if (results['QED'][i] >= 0.6 and results['SA'][i] <= 4):\n",
    "                sum += 1\n",
    "\n",
    "        generated_molecules = [Chem.MolFromSmiles(s) for s in valid_answer_list]\n",
    "        original_molecules = [Chem.MolFromSmiles(s) for s in original_smiles]\n",
    "        # 计算多样性\n",
    "        diversity = calculate_diversity(generated_molecules)\n",
    "        # 计算距离\n",
    "        distance = calculate_distance(generated_molecules, original_molecules)\n",
    "\n",
    "        print('valid_ratio:', valid_ratio, 'uniqueness:', uniqueness, 'Quality:', sum / 100,\n",
    "              'SA:', SA_score / len(unique_smiles_lst) if len(unique_smiles_lst) != 0 else 0,\n",
    "              'QED:', QED_score / len(unique_smiles_lst) if len(unique_smiles_lst) != 0 else 0, 'diversity:', diversity,\n",
    "              'distance:', distance)\n",
    "        valid_ratio_sum += valid_ratio\n",
    "        uniqueness_sum += uniqueness\n",
    "        quality_sum += sum / 100\n",
    "        if len(unique_smiles_lst) == 0:\n",
    "            sa_sum += 0\n",
    "            qed_sum += 0\n",
    "        else:\n",
    "            sa_sum += SA_score / len(unique_smiles_lst)\n",
    "            qed_sum += QED_score / len(unique_smiles_lst)\n",
    "        div_sum += diversity\n",
    "        dist_sum += distance\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"运行时间: {elapsed_time:.4f} 秒\")\n",
    "    print(f\"valid_ratio_avg: {valid_ratio_sum / len(scaffold_lst)}, uniqueness_avg: {uniqueness_sum / len(scaffold_lst)}, \"\n",
    "          f\"quality_avg: {quality_sum / len(scaffold_lst)}, sa_avg: {sa_sum / len(scaffold_lst)}, \"\n",
    "          f\"qed_avg: {qed_sum / len(scaffold_lst)}, div_avg: {div_sum / len(scaffold_lst)}, dist_avg: {dist_sum / len(scaffold_lst)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main_scaffold1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44813282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fragpt",
   "language": "python",
   "name": "fragpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
